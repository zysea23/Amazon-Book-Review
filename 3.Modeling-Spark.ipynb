{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Models Comparison: ALS, SVD, and NMF\n",
    "\n",
    " This notebook compares three recommendation algorithms:\n",
    " 1. ALS (Alternating Least Squares) using Spark MLlib\n",
    " 2. SVD (Singular Value Decomposition) using scikit-learn\n",
    " 3. NMF (Non-negative Matrix Factorization) using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Import necessary libraries ---\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField, ArrayType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from scipy.sparse import csr_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "GCP_PROJECT_ID = \"review-analysis-456008\"  \n",
    "BQ_DATASET = \"amazon_reviews_dataset\" \n",
    "BQ_TRAIN_TABLE = \"train_reviews\"\n",
    "BQ_TEST_TABLE = \"test_reviews\"\n",
    "\n",
    "TEMP_GCS_BUCKET = \"review-data-yu/temp\" \n",
    "\n",
    "# Model Hyperparameters\n",
    "FACTORS = 50  # Number of latent factors for all models\n",
    "MAX_ITER = 20  # Maximum iterations for all models\n",
    "REG_PARAM = 0.01  # Regularization parameter\n",
    "\n",
    "# Evaluation Parameters\n",
    "EVAL_K = 20  # Top-K recommendations for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Initialized and Configured.\n",
      "Using temporary GCS bucket: review-data-yu/temp\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Recommendation Models Comparison\") \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.29.0') \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure the BigQuery connector\n",
    "spark.conf.set(\"temporaryGcsBucket\", TEMP_GCS_BUCKET)\n",
    "spark.conf.set(\"viewsEnabled\", \"true\") \n",
    "\n",
    "print(\"Spark Session Initialized and Configured.\")\n",
    "print(f\"Using temporary GCS bucket: {TEMP_GCS_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: review-analysis-456008.amazon_reviews_dataset.train_reviews\n",
      "Loading test data from: review-analysis-456008.amazon_reviews_dataset.test_reviews\n",
      "Data loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 117283\n",
      "Testing data count: 26196\n"
     ]
    }
   ],
   "source": [
    "# Construct full table names\n",
    "train_table_id = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_TRAIN_TABLE}\"\n",
    "test_table_id = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_TEST_TABLE}\"\n",
    "\n",
    "print(f\"Loading training data from: {train_table_id}\")\n",
    "train_df = spark.read.format(\"bigquery\") \\\n",
    "    .option(\"table\", train_table_id) \\\n",
    "    .load() \\\n",
    "    .select(\"user_idx\", \"product_idx\", \"star_rating\") \\\n",
    "    .withColumn(\"user_idx\", F.col(\"user_idx\").cast(IntegerType())) \\\n",
    "    .withColumn(\"product_idx\", F.col(\"product_idx\").cast(IntegerType())) \\\n",
    "    .withColumn(\"star_rating\", F.col(\"star_rating\").cast(FloatType()))\n",
    "\n",
    "print(f\"Loading test data from: {test_table_id}\")\n",
    "test_df = spark.read.format(\"bigquery\") \\\n",
    "    .option(\"table\", test_table_id) \\\n",
    "    .load() \\\n",
    "    .select(\"user_idx\", \"product_idx\", \"star_rating\") \\\n",
    "    .withColumn(\"user_idx\", F.col(\"user_idx\").cast(IntegerType())) \\\n",
    "    .withColumn(\"product_idx\", F.col(\"product_idx\").cast(IntegerType())) \\\n",
    "    .withColumn(\"star_rating\", F.col(\"star_rating\").cast(FloatType()))\n",
    "\n",
    "# Cache dataframes for better performance\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Training data count: {train_df.count()}\")\n",
    "print(f\"Testing data count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model 1: ALS (Spark MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training ALS Model ---\n",
      "Training ALS model with factors=50, maxIter=20, regParam=0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training completed in 35.23 seconds.\n",
      "Generating Top-20 recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ALS on 5415 users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 421:==========================================>          (162 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ALS Model Evaluation ---\n",
      "Recall@20: 0.0236\n",
      "NDCG@20:   0.0139\n",
      "Training time:   35.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# --- Model 1: Alternating Least Squares (ALS) using Spark MLlib ---\n",
    "print(\"\\n--- Training ALS Model ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate the ALS model\n",
    "als = ALS(\n",
    "    rank=FACTORS,\n",
    "    maxIter=MAX_ITER,\n",
    "    regParam=REG_PARAM,\n",
    "    userCol=\"user_idx\",\n",
    "    itemCol=\"product_idx\",\n",
    "    ratingCol=\"star_rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    implicitPrefs=False\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "print(f\"Training ALS model with factors={FACTORS}, maxIter={MAX_ITER}, regParam={REG_PARAM}...\")\n",
    "als_model = als.fit(train_df)\n",
    "als_training_time = time.time() - start_time\n",
    "print(f\"ALS model training completed in {als_training_time:.2f} seconds.\")\n",
    "\n",
    "# Prepare ground truth data\n",
    "ground_truth_df = test_df \\\n",
    "    .groupBy(\"user_idx\") \\\n",
    "    .agg(F.collect_list(\"product_idx\").alias(\"actual_items\"))\n",
    "ground_truth_df.cache()\n",
    "\n",
    "# Generate recommendations\n",
    "print(f\"Generating Top-{EVAL_K} recommendations...\")\n",
    "als_recs_df = als_model.recommendForAllUsers(EVAL_K) \\\n",
    "    .withColumn(\"recommendations\", F.expr(\"transform(recommendations, x -> x.product_idx)\")) \\\n",
    "    .select(\"user_idx\", F.col(\"recommendations\").alias(\"predicted_items\"))\n",
    "als_recs_df.cache()\n",
    "\n",
    "# Prepare evaluation DataFrame\n",
    "als_eval_df = ground_truth_df.join(als_recs_df, \"user_idx\", \"inner\")\n",
    "als_eval_df = als_eval_df.withColumn(\"predicted_items_double\", \n",
    "                               F.expr(\"transform(predicted_items, x -> cast(x as double))\"))\n",
    "als_eval_df = als_eval_df.withColumn(\"actual_items_double\", \n",
    "                               F.expr(\"transform(actual_items, x -> cast(x as double))\"))\n",
    "als_eval_df.cache()\n",
    "num_als_eval_users = als_eval_df.count()\n",
    "print(f\"Evaluating ALS on {num_als_eval_users} users\")\n",
    "\n",
    "# Initialize evaluators\n",
    "recall_evaluator = RankingEvaluator(\n",
    "    metricName=\"recallAtK\",\n",
    "    k=EVAL_K,\n",
    "    predictionCol=\"predicted_items_double\", \n",
    "    labelCol=\"actual_items_double\"          \n",
    ")\n",
    "\n",
    "ndcg_evaluator = RankingEvaluator(\n",
    "    metricName=\"ndcgAtK\",\n",
    "    k=EVAL_K,\n",
    "    predictionCol=\"predicted_items_double\", \n",
    "    labelCol=\"actual_items_double\"          \n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "als_recall = recall_evaluator.evaluate(als_eval_df)\n",
    "als_ndcg = ndcg_evaluator.evaluate(als_eval_df)\n",
    "\n",
    "print(\"\\n--- ALS Model Evaluation ---\")\n",
    "print(f\"Recall@{EVAL_K}: {als_recall:.4f}\")\n",
    "print(f\"NDCG@{EVAL_K}:   {als_ndcg:.4f}\")\n",
    "print(f\"Training time:   {als_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convert Data to Matrix Format for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting data for sklearn-based models...\n",
      "Using 117283 training samples and 26196 test samples\n",
      "Created user-item matrix with shape: (6238, 10957)\n"
     ]
    }
   ],
   "source": [
    "# --- Convert data for sklearn-based models ---\n",
    "# Convert Spark DataFrames to Pandas for using with sklearn\n",
    "print(\"\\nConverting data for sklearn-based models...\")\n",
    "\n",
    "# Take a sample if the dataset is too large\n",
    "# Adjust the sampling fraction based on your data size and available memory\n",
    "sample_fraction = 1.0  # Use all data, reduce if needed\n",
    "\n",
    "# Convert to pandas dataframes\n",
    "train_pandas = train_df.sample(fraction=sample_fraction, seed=42).toPandas()\n",
    "test_pandas = test_df.sample(fraction=sample_fraction, seed=42).toPandas()\n",
    "\n",
    "print(f\"Using {len(train_pandas)} training samples and {len(test_pandas)} test samples\")\n",
    "\n",
    "# Get unique user and item indices\n",
    "users = sorted(train_pandas['user_idx'].unique())\n",
    "items = sorted(train_pandas['product_idx'].unique())\n",
    "\n",
    "# Create user and item id to position mappings\n",
    "user_to_idx = {user: i for i, user in enumerate(users)}\n",
    "item_to_idx = {item: i for i, item in enumerate(items)}\n",
    "idx_to_item = {i: item for item, i in item_to_idx.items()}\n",
    "\n",
    "# Create test user-item pairs for evaluation\n",
    "test_user_item_pairs = {}\n",
    "for _, row in test_pandas.iterrows():\n",
    "    user = row['user_idx']\n",
    "    item = row['product_idx']\n",
    "    if user not in test_user_item_pairs:\n",
    "        test_user_item_pairs[user] = []\n",
    "    test_user_item_pairs[user].append(item)\n",
    "\n",
    "# Build user-item rating matrix (sparse matrix)\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "# Create sparse matrix for training data\n",
    "ratings = []\n",
    "row_ind = []\n",
    "col_ind = []\n",
    "\n",
    "for _, row in train_pandas.iterrows():\n",
    "    user_idx = user_to_idx.get(row['user_idx'], -1)\n",
    "    item_idx = item_to_idx.get(row['product_idx'], -1)\n",
    "    \n",
    "    if user_idx != -1 and item_idx != -1:\n",
    "        ratings.append(row['star_rating'])\n",
    "        row_ind.append(user_idx)\n",
    "        col_ind.append(item_idx)\n",
    "\n",
    "# Create sparse matrix\n",
    "train_matrix = csr_matrix((ratings, (row_ind, col_ind)), shape=(n_users, n_items))\n",
    "\n",
    "print(f\"Created user-item matrix with shape: {train_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model 2: SVD (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training SVD Model ---\n",
      "SVD model training completed in 1.60 seconds.\n",
      "Generating SVD Top-20 recommendations...\n",
      "\n",
      "--- SVD Model Evaluation ---\n",
      "Number of users evaluated: 5415\n",
      "Recall@20: 0.0710\n",
      "NDCG@20:   0.0443\n",
      "Training time:   1.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Model 2: Singular Value Decomposition (SVD) using scikit-learn ---\n",
    "print(\"\\n--- Training SVD Model ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train SVD model\n",
    "svd = TruncatedSVD(n_components=FACTORS, n_iter=MAX_ITER, random_state=42)\n",
    "user_factors = svd.fit_transform(train_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "svd_training_time = time.time() - start_time\n",
    "print(f\"SVD model training completed in {svd_training_time:.2f} seconds.\")\n",
    "\n",
    "# Function to generate recommendations\n",
    "def get_svd_recommendations(user_idx, n=10):\n",
    "    \"\"\"Generate top-N recommendations for a user based on SVD factors\"\"\"\n",
    "    if user_idx not in user_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_vector = user_factors[user_to_idx[user_idx]]\n",
    "    # Calculate predicted ratings\n",
    "    scores = np.dot(user_vector, item_factors.T)\n",
    "    # Get top N items\n",
    "    top_items_idx = np.argsort(-scores)[:n]\n",
    "    # Convert back to original item IDs\n",
    "    return [idx_to_item[idx] for idx in top_items_idx]\n",
    "\n",
    "# Generate recommendations for users in test set\n",
    "print(f\"Generating SVD Top-{EVAL_K} recommendations...\")\n",
    "svd_recommendations = {}\n",
    "for user_id in test_user_item_pairs.keys():\n",
    "    if user_id in user_to_idx:\n",
    "        svd_recommendations[user_id] = get_svd_recommendations(user_id, n=EVAL_K)\n",
    "\n",
    "# Prepare data for evaluation\n",
    "svd_eval_data = []\n",
    "for user_id, recommendations in svd_recommendations.items():\n",
    "    if user_id in test_user_item_pairs:\n",
    "        actual_items = test_user_item_pairs[user_id]\n",
    "        svd_eval_data.append({\n",
    "            'user_idx': user_id,\n",
    "            'predicted_items': recommendations,\n",
    "            'actual_items': actual_items\n",
    "        })\n",
    "\n",
    "# Functions to calculate metrics\n",
    "def calculate_recall_at_k(recommended_items, actual_items, k=10):\n",
    "    \"\"\"Calculate Recall@K metric\"\"\"\n",
    "    if not actual_items:\n",
    "        return 0.0\n",
    "    \n",
    "    hits = len(set(recommended_items[:k]) & set(actual_items))\n",
    "    return hits / len(actual_items)\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"Calculate DCG@K\"\"\"\n",
    "    r = np.asarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def calculate_ndcg_at_k(recommended_items, actual_items, k=10):\n",
    "    \"\"\"Calculate NDCG@K metric\"\"\"\n",
    "    if not actual_items:\n",
    "        return 0.0\n",
    "    \n",
    "    # Binary relevance (1 if item is in actual_items, 0 otherwise)\n",
    "    relevance = [1 if item in actual_items else 0 for item in recommended_items[:k]]\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = dcg_at_k(relevance, k)\n",
    "    \n",
    "    # Calculate ideal DCG\n",
    "    ideal_relevance = [1] * min(len(actual_items), k)\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Calculate metrics for SVD\n",
    "svd_recalls = []\n",
    "svd_ndcgs = []\n",
    "\n",
    "for data in svd_eval_data:\n",
    "    svd_recalls.append(calculate_recall_at_k(data['predicted_items'], data['actual_items'], k=EVAL_K))\n",
    "    svd_ndcgs.append(calculate_ndcg_at_k(data['predicted_items'], data['actual_items'], k=EVAL_K))\n",
    "\n",
    "svd_recall = np.mean(svd_recalls) if svd_recalls else 0\n",
    "svd_ndcg = np.mean(svd_ndcgs) if svd_ndcgs else 0\n",
    "\n",
    "print(\"\\n--- SVD Model Evaluation ---\")\n",
    "print(f\"Number of users evaluated: {len(svd_eval_data)}\")\n",
    "print(f\"Recall@{EVAL_K}: {svd_recall:.4f}\")\n",
    "print(f\"NDCG@{EVAL_K}:   {svd_ndcg:.4f}\")\n",
    "print(f\"Training time:   {svd_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model 3: NMF (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training NMF Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/micromamba/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 20 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF model training completed in 3.03 seconds.\n",
      "Generating NMF Top-20 recommendations...\n",
      "\n",
      "--- NMF Model Evaluation ---\n",
      "Number of users evaluated: 5415\n",
      "Recall@20: 0.0424\n",
      "NDCG@20:   0.0272\n",
      "Training time:   3.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Model 3: Non-negative Matrix Factorization (NMF) using scikit-learn ---\n",
    "print(\"\\n--- Training NMF Model ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train NMF model with correct parameters\n",
    "# Note: sklearn NMF uses alpha_W and alpha_H for regularization, not alpha\n",
    "nmf = NMF(n_components=FACTORS, max_iter=MAX_ITER, random_state=42, \n",
    "          alpha_W=REG_PARAM, alpha_H=REG_PARAM)\n",
    "user_factors_nmf = nmf.fit_transform(train_matrix)\n",
    "item_factors_nmf = nmf.components_.T\n",
    "\n",
    "nmf_training_time = time.time() - start_time\n",
    "print(f\"NMF model training completed in {nmf_training_time:.2f} seconds.\")\n",
    "\n",
    "# Function to generate recommendations\n",
    "def get_nmf_recommendations(user_idx, n=10):\n",
    "    \"\"\"Generate top-N recommendations for a user based on NMF factors\"\"\"\n",
    "    if user_idx not in user_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_vector = user_factors_nmf[user_to_idx[user_idx]]\n",
    "    # Calculate predicted ratings\n",
    "    scores = np.dot(user_vector, item_factors_nmf.T)\n",
    "    # Get top N items\n",
    "    top_items_idx = np.argsort(-scores)[:n]\n",
    "    # Convert back to original item IDs\n",
    "    return [idx_to_item[idx] for idx in top_items_idx]\n",
    "\n",
    "# Generate recommendations for users in test set\n",
    "print(f\"Generating NMF Top-{EVAL_K} recommendations...\")\n",
    "nmf_recommendations = {}\n",
    "for user_id in test_user_item_pairs.keys():\n",
    "    if user_id in user_to_idx:\n",
    "        nmf_recommendations[user_id] = get_nmf_recommendations(user_id, n=EVAL_K)\n",
    "\n",
    "# Prepare data for evaluation\n",
    "nmf_eval_data = []\n",
    "for user_id, recommendations in nmf_recommendations.items():\n",
    "    if user_id in test_user_item_pairs:\n",
    "        actual_items = test_user_item_pairs[user_id]\n",
    "        nmf_eval_data.append({\n",
    "            'user_idx': user_id,\n",
    "            'predicted_items': recommendations,\n",
    "            'actual_items': actual_items\n",
    "        })\n",
    "\n",
    "# Calculate metrics for NMF\n",
    "nmf_recalls = []\n",
    "nmf_ndcgs = []\n",
    "\n",
    "for data in nmf_eval_data:\n",
    "    nmf_recalls.append(calculate_recall_at_k(data['predicted_items'], data['actual_items'], k=EVAL_K))\n",
    "    nmf_ndcgs.append(calculate_ndcg_at_k(data['predicted_items'], data['actual_items'], k=EVAL_K))\n",
    "\n",
    "nmf_recall = np.mean(nmf_recalls) if nmf_recalls else 0\n",
    "nmf_ndcg = np.mean(nmf_ndcgs) if nmf_ndcgs else 0\n",
    "\n",
    "print(\"\\n--- NMF Model Evaluation ---\")\n",
    "print(f\"Number of users evaluated: {len(nmf_eval_data)}\")\n",
    "print(f\"Recall@{EVAL_K}: {nmf_recall:.4f}\")\n",
    "print(f\"NDCG@{EVAL_K}:   {nmf_ndcg:.4f}\")\n",
    "print(f\"Training time:   {nmf_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Comparison Summary ---\n",
      "  Model  Recall@20   NDCG@20  Training Time (s)\n",
      "0   ALS   0.023591  0.013878          35.230029\n",
      "1   SVD   0.070978  0.044329           1.596641\n",
      "2   NMF   0.042439  0.027186           3.031559\n",
      "\n",
      "Best model for Recall@20: SVD\n",
      "Best model for NDCG@20: SVD\n",
      "Fastest model to train: SVD\n"
     ]
    }
   ],
   "source": [
    "# --- Compare all models ---\n",
    "print(\"\\n--- Model Comparison Summary ---\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['ALS', 'SVD', 'NMF'],\n",
    "    f'Recall@{EVAL_K}': [als_recall, svd_recall, nmf_recall],\n",
    "    f'NDCG@{EVAL_K}': [als_ndcg, svd_ndcg, nmf_ndcg],\n",
    "    'Training Time (s)': [als_training_time, svd_training_time, nmf_training_time]\n",
    "})\n",
    "\n",
    "# Display comparison\n",
    "print(comparison_df)\n",
    "\n",
    "# Identify best model for each metric\n",
    "best_recall_model = comparison_df.loc[comparison_df[f'Recall@{EVAL_K}'].idxmax()]['Model']\n",
    "best_ndcg_model = comparison_df.loc[comparison_df[f'NDCG@{EVAL_K}'].idxmax()]['Model']\n",
    "fastest_model = comparison_df.loc[comparison_df['Training Time (s)'].idxmin()]['Model']\n",
    "\n",
    "print(f\"\\nBest model for Recall@{EVAL_K}: {best_recall_model}\")\n",
    "print(f\"Best model for NDCG@{EVAL_K}: {best_ndcg_model}\")\n",
    "print(f\"Fastest model to train: {fastest_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local) (Local)",
   "language": "python",
   "name": "micromamba-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
