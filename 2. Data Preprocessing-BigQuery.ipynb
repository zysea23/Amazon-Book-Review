{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5043a634-3368-4d07-9ee0-a1135984ddf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Amazon Book Review Data Preprocessing for Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8148ae1e-9af8-4160-83b2-4f0a8da9a0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from google.cloud import bigquery\n",
    "import apache_beam as beam\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "import os\n",
    "\n",
    "# Set GCP project and storage bucket variables\n",
    "PROJECT_ID = 'review-analysis-456008'\n",
    "BUCKET_NAME = 'review-data-yu'\n",
    "DATA_FILE = 'gs://review-data-yu/raw-data/amazon_reviews_us_Books_v1_02.tsv'\n",
    "\n",
    "# Define Beam pipeline options\n",
    "pipeline_options = PipelineOptions(\n",
    "    project=PROJECT_ID,\n",
    "    temp_location=f'gs://{BUCKET_NAME}/temp',\n",
    "    region='us-central1'\n",
    ")\n",
    "\n",
    "# Ensure we use DirectRunner (local execution)\n",
    "pipeline_options.view_as(StandardOptions).runner = 'DirectRunner'\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da5dc2-76d6-450b-b38a-ab22f278ab6f",
   "metadata": {},
   "source": [
    "## 1. Create BigQuery Dataset and Raw Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196595c8-f3cf-4ee6-8439-8b25e21eaf06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset review-analysis-456008.amazon_reviews_dataset created or already exists.\n",
      "Table review-analysis-456008.amazon_reviews_dataset.raw_reviews created or already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create a BigQuery dataset\n",
    "dataset_id = f\"{PROJECT_ID}.amazon_reviews_dataset\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"us-central1\"\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Dataset {dataset_id} created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "\n",
    "# Define table schema based on TSV structure\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"marketplace\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"customer_id\", \"INTEGER\"),  # INTEGER based on the data type\n",
    "    bigquery.SchemaField(\"review_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"product_parent\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"product_title\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"product_category\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"star_rating\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"helpful_votes\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"total_votes\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"vine\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"verified_purchase\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"review_headline\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"review_body\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"review_date\", \"STRING\")\n",
    "]\n",
    "\n",
    "# Create table for raw data\n",
    "table_id = f\"{dataset_id}.raw_reviews\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "try:\n",
    "    table = client.create_table(table, exists_ok=True)\n",
    "    print(f\"Table {table_id} created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb5f55-a37b-45a9-bf31-5f85f714cb79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load TSV Data into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4215957-d1cb-4d95-ad8c-ee677ea10498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3105520 rows into review-analysis-456008.amazon_reviews_dataset.raw_reviews\n"
     ]
    }
   ],
   "source": [
    "# Configure data loading job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    field_delimiter='\\t',\n",
    "    quote_character='',  # Disable quote handling for TSV\n",
    "    allow_quoted_newlines=True,  # Allow newlines in quoted fields\n",
    "    max_bad_records=10000,  # Allow some bad records\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Start the data loading job\n",
    "try:\n",
    "    load_job = client.load_table_from_uri(\n",
    "        DATA_FILE,  # GCS TSV file path\n",
    "        table_id,   # Target table\n",
    "        job_config=job_config\n",
    "    )\n",
    "    \n",
    "    # Wait for the job to complete\n",
    "    load_job.result()\n",
    "    \n",
    "    print(f\"Loaded {load_job.output_rows} rows into {table_id}\")\n",
    "    \n",
    "    # Check for errors\n",
    "    if load_job.errors:\n",
    "        print(f\"Encountered {len(load_job.errors)} errors:\")\n",
    "        for error in load_job.errors[:5]:  # Show only first 5 errors\n",
    "            print(f\"  - {error}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b2848-285a-4693-8735-9a7bb515ba72",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis using BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f661531-ad8e-4379-a7b6-7d526d802b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BASIC STATISTICS =====\n",
      "Total Records: 3105520\n",
      "Unique Users: 1502380\n",
      "Unique Products: 779733\n",
      "Average Rating: 4.182723022231384\n",
      "\n",
      "===== RATING DISTRIBUTION =====\n",
      "star_rating | count | percentage\n",
      "------------------------------\n",
      "1 | 238221 | 7.67%\n",
      "2 | 166384 | 5.36%\n",
      "3 | 249926 | 8.05%\n",
      "4 | 586182 | 18.88%\n",
      "5 | 1864807 | 60.05%\n",
      "\n",
      "===== USER ENGAGEMENT ANALYSIS =====\n",
      "Total Unique Customers: 1502380\n",
      "Average Reviews per Customer: 2.067066920486164\n",
      "Max Reviews by a Single Customer: 21922\n",
      "Customers with Multiple Reviews: 370478 (24.66%)\n",
      "\n",
      "===== PRODUCT ANALYSIS =====\n",
      "Total Unique Products: 779733\n",
      "Average Reviews per Product: 3.9827992402527257\n",
      "Max Reviews for a Single Product: 4625\n",
      "Products with Multiple Reviews: 380957 (48.86%)\n"
     ]
    }
   ],
   "source": [
    "# Perform basic data statistics\n",
    "basic_stats_query = f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as total_records,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products,\n",
    "  AVG(star_rating) as avg_rating\n",
    "FROM `{table_id}`\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and get results without using to_dataframe()\n",
    "basic_stats_job = client.query(basic_stats_query)\n",
    "basic_stats_result = list(basic_stats_job)\n",
    "\n",
    "print(\"\\n===== BASIC STATISTICS =====\")\n",
    "if basic_stats_result:\n",
    "    row = basic_stats_result[0]\n",
    "    print(f\"Total Records: {row['total_records']}\")\n",
    "    print(f\"Unique Users: {row['unique_users']}\")\n",
    "    print(f\"Unique Products: {row['unique_products']}\")\n",
    "    print(f\"Average Rating: {row['avg_rating']}\")\n",
    "\n",
    "# Rating distribution analysis\n",
    "rating_query = f\"\"\"\n",
    "SELECT \n",
    "  star_rating,\n",
    "  COUNT(*) as count,\n",
    "  ROUND((COUNT(*) / (SELECT COUNT(*) FROM `{table_id}`)) * 100, 2) as percentage\n",
    "FROM `{table_id}`\n",
    "GROUP BY star_rating\n",
    "ORDER BY star_rating\n",
    "\"\"\"\n",
    "\n",
    "rating_job = client.query(rating_query)\n",
    "rating_results = list(rating_job)\n",
    "\n",
    "print(\"\\n===== RATING DISTRIBUTION =====\")\n",
    "print(\"star_rating | count | percentage\")\n",
    "print(\"------------------------------\")\n",
    "for row in rating_results:\n",
    "    print(f\"{row['star_rating']} | {row['count']} | {row['percentage']}%\")\n",
    "\n",
    "# User engagement analysis\n",
    "user_query = f\"\"\"\n",
    "WITH user_reviews AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as review_count\n",
    "  FROM `{table_id}`\n",
    "  GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "  COUNT(*) as total_unique_customers,\n",
    "  AVG(review_count) as avg_reviews_per_customer,\n",
    "  MAX(review_count) as max_reviews_by_customer,\n",
    "  COUNT(CASE WHEN review_count > 1 THEN 1 END) as customers_with_multiple_reviews,\n",
    "  ROUND(COUNT(CASE WHEN review_count > 1 THEN 1 END) / COUNT(*) * 100, 2) as multiple_reviews_percentage\n",
    "FROM user_reviews\n",
    "\"\"\"\n",
    "\n",
    "user_job = client.query(user_query)\n",
    "user_results = list(user_job)\n",
    "\n",
    "print(\"\\n===== USER ENGAGEMENT ANALYSIS =====\")\n",
    "if user_results:\n",
    "    row = user_results[0]\n",
    "    print(f\"Total Unique Customers: {row['total_unique_customers']}\")\n",
    "    print(f\"Average Reviews per Customer: {row['avg_reviews_per_customer']}\")\n",
    "    print(f\"Max Reviews by a Single Customer: {row['max_reviews_by_customer']}\")\n",
    "    print(f\"Customers with Multiple Reviews: {row['customers_with_multiple_reviews']} ({row['multiple_reviews_percentage']}%)\")\n",
    "\n",
    "# Product analysis\n",
    "product_query = f\"\"\"\n",
    "WITH product_reviews AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    COUNT(*) as review_count\n",
    "  FROM `{table_id}`\n",
    "  GROUP BY product_id\n",
    ")\n",
    "SELECT \n",
    "  COUNT(*) as total_unique_products,\n",
    "  AVG(review_count) as avg_reviews_per_product,\n",
    "  MAX(review_count) as max_reviews_by_product,\n",
    "  COUNT(CASE WHEN review_count > 1 THEN 1 END) as products_with_multiple_reviews,\n",
    "  ROUND(COUNT(CASE WHEN review_count > 1 THEN 1 END) / COUNT(*) * 100, 2) as multiple_reviews_percentage\n",
    "FROM product_reviews\n",
    "\"\"\"\n",
    "\n",
    "product_job = client.query(product_query)\n",
    "product_results = list(product_job)\n",
    "\n",
    "print(\"\\n===== PRODUCT ANALYSIS =====\")\n",
    "if product_results:\n",
    "    row = product_results[0]\n",
    "    print(f\"Total Unique Products: {row['total_unique_products']}\")\n",
    "    print(f\"Average Reviews per Product: {row['avg_reviews_per_product']}\")\n",
    "    print(f\"Max Reviews for a Single Product: {row['max_reviews_by_product']}\")\n",
    "    print(f\"Products with Multiple Reviews: {row['products_with_multiple_reviews']} ({row['multiple_reviews_percentage']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64667de4-4a3d-496f-8014-7988f2e9073c",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8878d3-34f6-4d7f-b62f-e30d7028b14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FILTERING HIGH-FREQUENCY USERS AND PRODUCTS =====\n",
      "High-frequency users table created.\n",
      "High-frequency products table created.\n",
      "Found 6521 users with ≥30 reviews\n",
      "Found 11607 products with ≥30 reviews\n"
     ]
    }
   ],
   "source": [
    "# Create tables for high-frequency users and products\n",
    "print(\"\\n===== FILTERING HIGH-FREQUENCY USERS AND PRODUCTS =====\")\n",
    "\n",
    "# Create table for high-frequency users (≥30 reviews)\n",
    "high_freq_users_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.high_freq_users` AS\n",
    "SELECT customer_id, COUNT(*) as review_count\n",
    "FROM `{table_id}`\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) >= 30\n",
    "\"\"\"\n",
    "\n",
    "# Create table for high-frequency products (≥30 reviews)\n",
    "high_freq_products_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.high_freq_products` AS\n",
    "SELECT product_id, COUNT(*) as review_count\n",
    "FROM `{table_id}`\n",
    "GROUP BY product_id\n",
    "HAVING COUNT(*) >= 30\n",
    "\"\"\"\n",
    "\n",
    "# Execute the queries\n",
    "try:\n",
    "    client.query(high_freq_users_query).result()\n",
    "    print(\"High-frequency users table created.\")\n",
    "    \n",
    "    client.query(high_freq_products_query).result()\n",
    "    print(\"High-frequency products table created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating frequency tables: {e}\")\n",
    "\n",
    "# Count how many users and products meet the criteria\n",
    "count_query = f\"\"\"\n",
    "SELECT\n",
    "  (SELECT COUNT(*) FROM `{dataset_id}.high_freq_users`) as high_freq_users,\n",
    "  (SELECT COUNT(*) FROM `{dataset_id}.high_freq_products`) as high_freq_products\n",
    "\"\"\"\n",
    "\n",
    "count_job = client.query(count_query)\n",
    "count_results = list(count_job)\n",
    "\n",
    "if count_results:\n",
    "    row = count_results[0]\n",
    "    print(f\"Found {row['high_freq_users']} users with ≥30 reviews\")\n",
    "    print(f\"Found {row['high_freq_products']} products with ≥30 reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dad60-45e1-418e-b251-7860cc1199db",
   "metadata": {},
   "source": [
    "## 5. Create Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b064838-8b2e-4ca2-9836-f1afbe81bd97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered reviews table created.\n",
      "Filtered dataset contains 143479 reviews\n"
     ]
    }
   ],
   "source": [
    "# Create filtered dataset with only high-frequency users and products\n",
    "filtered_reviews_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.filtered_reviews` AS\n",
    "SELECT r.*\n",
    "FROM `{table_id}` r\n",
    "JOIN `{dataset_id}.high_freq_users` u ON r.customer_id = u.customer_id\n",
    "JOIN `{dataset_id}.high_freq_products` p ON r.product_id = p.product_id\n",
    "WHERE r.star_rating IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    client.query(filtered_reviews_query).result()\n",
    "    print(\"Filtered reviews table created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating filtered table: {e}\")\n",
    "\n",
    "# Count records in filtered dataset\n",
    "filtered_count_query = f\"\"\"\n",
    "SELECT COUNT(*) as filtered_count\n",
    "FROM `{dataset_id}.filtered_reviews`\n",
    "\"\"\"\n",
    "\n",
    "filtered_count_job = client.query(filtered_count_query)\n",
    "filtered_count_results = list(filtered_count_job)\n",
    "\n",
    "if filtered_count_results:\n",
    "    row = filtered_count_results[0]\n",
    "    print(f\"Filtered dataset contains {row['filtered_count']} reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeb523-f5c5-49e7-bae7-19d6d44d7730",
   "metadata": {},
   "source": [
    "## 6. Create ID Mappings for Users and Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f185fdf9-9806-4142-b99f-70697fd4f410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID mapping table created.\n",
      "Product ID mapping table created.\n",
      "Created mappings for 6238 users and 11079 products\n"
     ]
    }
   ],
   "source": [
    "# Create user ID mapping table\n",
    "user_mapping_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.user_id_mapping` AS\n",
    "SELECT \n",
    "  customer_id,\n",
    "  ROW_NUMBER() OVER (ORDER BY customer_id) - 1 as user_idx\n",
    "FROM (\n",
    "  SELECT DISTINCT customer_id\n",
    "  FROM `{dataset_id}.filtered_reviews`\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create product ID mapping table\n",
    "product_mapping_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.product_id_mapping` AS\n",
    "SELECT \n",
    "  product_id,\n",
    "  ROW_NUMBER() OVER (ORDER BY product_id) - 1 as product_idx\n",
    "FROM (\n",
    "  SELECT DISTINCT product_id\n",
    "  FROM `{dataset_id}.filtered_reviews`\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    client.query(user_mapping_query).result()\n",
    "    print(\"User ID mapping table created.\")\n",
    "    \n",
    "    client.query(product_mapping_query).result()\n",
    "    print(\"Product ID mapping table created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating mapping tables: {e}\")\n",
    "\n",
    "# Count mappings\n",
    "mapping_count_query = f\"\"\"\n",
    "SELECT\n",
    "  (SELECT COUNT(*) FROM `{dataset_id}.user_id_mapping`) as user_count,\n",
    "  (SELECT COUNT(*) FROM `{dataset_id}.product_id_mapping`) as product_count\n",
    "\"\"\"\n",
    "\n",
    "mapping_count_job = client.query(mapping_count_query)\n",
    "mapping_count_results = list(mapping_count_job)\n",
    "\n",
    "if mapping_count_results:\n",
    "    row = mapping_count_results[0]\n",
    "    print(f\"Created mappings for {row['user_count']} users and {row['product_count']} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f29bd-942a-4179-ade2-288b80223b48",
   "metadata": {},
   "source": [
    "## 7. Apply ID Mappings to Create Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86505008-14c9-4541-916f-b8a8409abece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed reviews table created with ID mappings applied.\n",
      "Processed dataset contains 143479 reviews\n"
     ]
    }
   ],
   "source": [
    "# Create processed dataset with numeric indices\n",
    "processed_reviews_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.processed_reviews` AS\n",
    "SELECT \n",
    "  f.*,\n",
    "  u.user_idx,\n",
    "  p.product_idx\n",
    "FROM `{dataset_id}.filtered_reviews` f\n",
    "JOIN `{dataset_id}.user_id_mapping` u ON f.customer_id = u.customer_id\n",
    "JOIN `{dataset_id}.product_id_mapping` p ON f.product_id = p.product_id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    client.query(processed_reviews_query).result()\n",
    "    print(\"Processed reviews table created with ID mappings applied.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating processed table: {e}\")\n",
    "\n",
    "# Verify processed dataset\n",
    "processed_count_query = f\"\"\"\n",
    "SELECT COUNT(*) as processed_count\n",
    "FROM `{dataset_id}.processed_reviews`\n",
    "\"\"\"\n",
    "\n",
    "processed_count_job = client.query(processed_count_query)\n",
    "processed_count_results = list(processed_count_job)\n",
    "\n",
    "if processed_count_results:\n",
    "    row = processed_count_results[0]\n",
    "    print(f\"Processed dataset contains {row['processed_count']} reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c5d70-1d12-4990-bc12-77a334f02ab3",
   "metadata": {},
   "source": [
    "## 8. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fc78204-6eb3-4f77-b747-4efc73c709d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary table with random values created.\n",
      "Training set created.\n",
      "Test set created.\n",
      "Train/test split completed.\n"
     ]
    }
   ],
   "source": [
    "# First, create a temporary table with random values\n",
    "random_values_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.temp_reviews_random` AS\n",
    "SELECT \n",
    "  *,\n",
    "  RAND() as random_value,\n",
    "  ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY RAND()) as user_review_rank,\n",
    "  COUNT(*) OVER(PARTITION BY customer_id) as user_review_count\n",
    "FROM `{dataset_id}.processed_reviews`\n",
    "\"\"\"\n",
    "\n",
    "# Then create training set from the temporary table\n",
    "train_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.train_reviews` AS\n",
    "SELECT * EXCEPT(random_value, user_review_rank, user_review_count)\n",
    "FROM `{dataset_id}.temp_reviews_random`\n",
    "WHERE user_review_rank <= CEIL(0.8 * user_review_count)\n",
    "\"\"\"\n",
    "\n",
    "# Create test set from the remaining reviews\n",
    "test_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{dataset_id}.test_reviews` AS\n",
    "SELECT *\n",
    "FROM `{dataset_id}.processed_reviews`\n",
    "WHERE review_id NOT IN (\n",
    "  SELECT review_id FROM `{dataset_id}.train_reviews`\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Step 1: Create temporary table with random values\n",
    "    client.query(random_values_query).result()\n",
    "    print(\"Temporary table with random values created.\")\n",
    "    \n",
    "    # Step 2: Create training set\n",
    "    client.query(train_query).result()\n",
    "    print(\"Training set created.\")\n",
    "    \n",
    "    # Step 3: Create test set\n",
    "    client.query(test_query).result()\n",
    "    print(\"Test set created.\")\n",
    "    \n",
    "    print(\"Train/test split completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating train/test split: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e33af8-661b-4a71-bb08-a80a0159fcc5",
   "metadata": {},
   "source": [
    "## 9. Validate Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d606ad9-890c-4084-8ed2-65733186dc93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DATA PROCESSING VALIDATION =====\n",
      "Stage | Count | Unique Users | Unique Products\n",
      "------------------------------------------\n",
      "raw | 3105520 | 1502380 | 779733\n",
      "filtered | 143479 | 6238 | 11079\n",
      "processed | 143479 | 6238 | 11079\n",
      "train | 117283 | 6238 | 10957\n",
      "test | 26196 | 5415 | 8302\n",
      "\n",
      "===== INDEX MAPPING VALIDATION =====\n",
      "User index range: 0 to 6237 (6238 unique)\n",
      "Product index range: 0 to 11078 (11079 unique)\n",
      "\n",
      "Data preprocessing complete! The processed data is available in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "# Final validation to ensure data quality throughout the pipeline\n",
    "validation_query = f\"\"\"\n",
    "SELECT \n",
    "  'raw' as stage, COUNT(*) as count,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products\n",
    "FROM `{table_id}`\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'filtered' as stage, COUNT(*) as count,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products\n",
    "FROM `{dataset_id}.filtered_reviews`\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'processed' as stage, COUNT(*) as count,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products  \n",
    "FROM `{dataset_id}.processed_reviews`\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'train' as stage, COUNT(*) as count,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products  \n",
    "FROM `{dataset_id}.train_reviews`\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'test' as stage, COUNT(*) as count,\n",
    "  COUNT(DISTINCT customer_id) as unique_users,\n",
    "  COUNT(DISTINCT product_id) as unique_products  \n",
    "FROM `{dataset_id}.test_reviews`\n",
    "ORDER BY \n",
    "  CASE \n",
    "    WHEN stage = 'raw' THEN 1\n",
    "    WHEN stage = 'filtered' THEN 2\n",
    "    WHEN stage = 'processed' THEN 3\n",
    "    WHEN stage = 'train' THEN 4\n",
    "    WHEN stage = 'test' THEN 5\n",
    "  END\n",
    "\"\"\"\n",
    "\n",
    "validation_job = client.query(validation_query)\n",
    "validation_results = list(validation_job)\n",
    "\n",
    "print(\"\\n===== DATA PROCESSING VALIDATION =====\")\n",
    "print(\"Stage | Count | Unique Users | Unique Products\")\n",
    "print(\"------------------------------------------\")\n",
    "for row in validation_results:\n",
    "    print(f\"{row['stage']} | {row['count']} | {row['unique_users']} | {row['unique_products']}\")\n",
    "\n",
    "# Additional quality check - verify user_idx and product_idx distributions\n",
    "idx_check_query = f\"\"\"\n",
    "SELECT \n",
    "  MIN(user_idx) as min_user_idx,\n",
    "  MAX(user_idx) as max_user_idx,\n",
    "  MIN(product_idx) as min_product_idx,\n",
    "  MAX(product_idx) as max_product_idx,\n",
    "  COUNT(DISTINCT user_idx) as unique_user_idx,\n",
    "  COUNT(DISTINCT product_idx) as unique_product_idx\n",
    "FROM `{dataset_id}.processed_reviews`\n",
    "\"\"\"\n",
    "\n",
    "idx_check_job = client.query(idx_check_query)\n",
    "idx_check_results = list(idx_check_job)\n",
    "\n",
    "print(\"\\n===== INDEX MAPPING VALIDATION =====\")\n",
    "if idx_check_results:\n",
    "    row = idx_check_results[0]\n",
    "    print(f\"User index range: {row['min_user_idx']} to {row['max_user_idx']} ({row['unique_user_idx']} unique)\")\n",
    "    print(f\"Product index range: {row['min_product_idx']} to {row['max_product_idx']} ({row['unique_product_idx']} unique)\")\n",
    "\n",
    "print(\"\\nData preprocessing complete! The processed data is available in BigQuery.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
